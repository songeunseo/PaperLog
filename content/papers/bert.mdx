---
title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
authors: "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
date: "2018-10-11"
lastmod: "2023-07-25"
publishedDate: "2018-10-11"
paperLink: "https://arxiv.org/abs/1810.04805"
categories: ["nlp", "language-model"]
tags: ["transformer", "pre-training", "fine-tuning", "bert"]
summary: "BERT는 양방향 트랜스포머 인코더를 활용한 사전 학습 모델로, 다양한 NLP 작업에서 획기적인 성능을 달성했습니다."
---

# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## 개요

BERT(Bidirectional Encoder Representations from Transformers)는 Google AI Language 팀이 2018년 발표한 논문으로, 자연어 처리(NLP) 분야에서 혁신적인 성능을 보여준 모델입니다. BERT는 Transformer 아키텍처의 인코더 부분만을 사용하여 양방향(bidirectional) 문맥을 학습하는 대규모 언어 모델입니다.

## 핵심 아이디어

BERT의 주요 혁신은 다음과 같습니다:

1. **양방향 문맥 학습**: 기존 언어 모델들이 텍스트를 왼쪽에서 오른쪽으로(또는 그 반대로) 처리했던 것과 달리, BERT는 문장의 전체 문맥을 양방향으로 고려하여 단어 표현을 학습합니다.

2. **사전 학습과 미세조정의 두 단계 프로세스**:
   - **사전 학습(Pre-training)**: 대량의 레이블이 없는 텍스트 데이터에서 일반적인 언어 이해 능력을 학습
   - **미세조정(Fine-tuning)**: 특정 태스크에 맞게 모델을 조정

3. **새로운 사전 학습 태스크**:
   - **마스킹된 언어 모델(MLM, Masked Language Model)**: 입력 문장의 일부 토큰을 마스킹(가리기)하고 원래 토큰을 예측하는 태스크
   - **다음 문장 예측(NSP, Next Sentence Prediction)**: 두 문장이 연속적인지 아닌지 예측하는 태스크

## 모델 구조

BERT는 기본적으로 Transformer 아키텍처의 인코더 부분을 사용합니다:

- **BERT-base**: 12개의 Transformer 블록, 768차원의 숨겨진 레이어, 12개의 어텐션 헤드, 총 110M 파라미터
- **BERT-large**: 24개의 Transformer 블록, 1024차원의 숨겨진 레이어, 16개의 어텐션 헤드, 총 340M 파라미터

## 주요 성과

BERT는 발표 당시 다음과 같은 다양한 NLP 태스크에서 최고 성능을 달성했습니다:

- GLUE 벤치마크: 80.5% (BERT-large)
- SQuAD v1.1(질의응답): F1 93.2%
- SWAG(상식 추론): 86.3%

## 영향 및 의의

BERT의 등장은 NLP 분야에 큰 변화를 가져왔습니다:

1. **전이 학습(Transfer Learning)의 효과 입증**: 대규모 데이터로 사전 학습된 모델을 다양한 태스크에 활용할 수 있음을 보여줌
2. **양방향 문맥의 중요성 증명**: 단어의 의미를 이해하기 위해서는 앞뒤 문맥을 모두 고려해야 함을 입증
3. **NLP 모델 개발 패러다임 변화**: 사전 학습 후 미세조정하는 방식이 표준이 됨

BERT는 이후 RoBERTa, ALBERT, DistilBERT, ELECTRA 등 수많은 후속 모델의 기반이 되었으며, NLP 분야의 발전을 가속화하는 데 크게 기여했습니다.

## 결론

BERT는 언어 모델의 사전 학습에 양방향성을 도입함으로써 자연어 이해 능력을 크게 향상시켰습니다. 다양한 NLP 태스크에서의 뛰어난 성능과 확장성으로 인해, BERT는 현대 NLP 연구와 응용의 핵심 기술로 자리 잡았습니다.

## 핵심 아이디어 요약

BERT(Bidirectional Encoder Representations from Transformers)는 구글 AI 연구팀이 2018년에 발표한 자연어 처리 모델로, 사전 학습(pre-training)과 미세 조정(fine-tuning)의 두 단계 접근 방식을 사용합니다. BERT의 가장 큰 혁신점은 양방향(bidirectional) 학습을 통해 문맥을 고려한 단어 표현을 생성한다는 것입니다.

기존의 언어 모델들은 주로 왼쪽에서 오른쪽(GPT)이나 오른쪽에서 왼쪽으로 텍스트를 처리했는데, BERT는 **마스크 언어 모델링(Masked Language Model, MLM)** 방식을 도입하여 양방향으로 문맥을 고려할 수 있게 했습니다. 이를 통해 더 풍부한 언어 이해가 가능해졌고, 다양한 자연어 처리 태스크에서 놀라운 성능 향상을 달성했습니다.

## 주요 포인트

- Transformer 인코더 구조를 기반으로 한 양방향 문맥 모델
- 두 가지 방식의 사전 학습: 마스크 언어 모델링(MLM)과 다음 문장 예측(NSP)
- 대규모 텍스트 코퍼스(BookCorpus + Wikipedia)에서 학습
- 특정 태스크를 위한 추가 레이어만 부착하여 미세 조정(fine-tuning) 가능
- 다양한 자연어 처리 태스크에서 최고 성능 달성

## 구조 설명

BERT는 Transformer 아키텍처의 인코더 부분을 사용합니다. 두 가지 크기의 모델이 제시되었습니다:

### BERT-Base

- 12개의 Transformer 레이어
- 12개의 어텐션 헤드
- 768 차원의 은닉 레이어
- 1억 1천만 개의 파라미터

### BERT-Large

- 24개의 Transformer 레이어
- 16개의 어텐션 헤드
- 1024 차원의 은닉 레이어
- 3억 4천만 개의 파라미터

## 사전 학습 방식

BERT는 다음 두 가지 비지도 학습 태스크로 사전 학습됩니다:

### 마스크 언어 모델링(Masked Language Model, MLM)

- 입력 텍스트의 약 15%를 무작위로 마스킹하고, 모델이 이를 예측하도록 학습
- 마스킹된 토큰의 80%는 [MASK] 토큰으로 대체
- 10%는 무작위 단어로 대체
- 10%는 원래 단어 그대로 유지

### 다음 문장 예측(Next Sentence Prediction, NSP)

- 두 문장이 주어졌을 때, 두 번째 문장이 실제로 첫 번째 문장 다음에 오는지 예측
- 입력은 [CLS] + 문장A + [SEP] + 문장B + [SEP] 형태로 구성

## 입력 표현

BERT의 입력 표현은 다음 세 가지 임베딩의 합으로 이루어집니다:

1. **토큰 임베딩**: 각 단어(토큰)의 의미 표현
2. **세그먼트 임베딩**: 어떤 문장에 속하는지 구분
3. **위치 임베딩**: 토큰의 위치 정보

## 실험 결과

BERT는 GLUE 벤치마크, SQuAD v1.1/v2.0, SWAG 등 다양한 자연어 처리 벤치마크에서 당시 최고 성능을 달성했습니다.

- **GLUE**: 80.5% (BERT-Large)로 이전 최고 성능 대비 7.7% 향상
- **SQuAD v1.1**: F1 점수 93.2% 달성
- **SQuAD v2.0**: F1 점수 83.1% 달성
- **SWAG**: 정확도 86.3% 달성

특히 BERT가 보여준 성능 향상은 적은 양의 레이블링된 데이터(low-resource scenario)에서도 효과적이었습니다.

## 내가 이해한 포인트

BERT의 핵심 아이디어는 언어 모델이 양방향으로 문맥을 이해할 수 있도록 하는 것입니다. 이는 마스크 언어 모델링이라는 새로운 사전 학습 방식을 통해 실현되었습니다.

또한 BERT는 사전 학습과 미세 조정의 두 단계 접근 방식을 효과적으로 활용했습니다. 대규모 텍스트 데이터에서 언어의 일반적인 표현을 학습한 후, 특정 태스크에 맞게 미세 조정함으로써 다양한 태스크에 활용할 수 있습니다. 이는 마치 컴퓨터 비전에서의 ImageNet 사전 학습과 유사한 패러다임 전환이었습니다.

BERT의 등장은 자연어 처리 분야에서 사전 학습된 언어 모델의 활용을 대중화시켰으며, 이후 등장한 많은 모델들(RoBERTa, ALBERT, DeBERTa 등)의 기반이 되었습니다. 현재도 다양한 산업 분야에서 BERT와 그 변형 모델들이 널리 사용되고 있습니다.

## 한줄 소감

> "양방향 문맥 학습을 가능하게 한 BERT는 자연어 처리 분야의 ImageNet 순간을 가져왔으며, 사전 학습-미세 조정 패러다임을 확립했다." 