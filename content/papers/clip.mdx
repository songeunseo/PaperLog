---
title: "Learning Transferable Visual Models From Natural Language Supervision"
authors: "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
date: "2023-07-05"
lastmod: "2023-07-20"
publishedDate: "2021-02-26"
paperLink: "https://arxiv.org/abs/2103.00020"
categories: ["multimodal", "vision"]
tags: ["clip", "contrastive-learning", "zero-shot", "multimodal-learning"]
summary: "CLIP은 자연어 설명을 통해 이미지를 이해하는 멀티모달 모델로, 다양한 시각적 인식 태스크에서 뛰어난 제로샷 성능을 보여줍니다."
---

# CLIP: 자연어 지도를 통한 전이 가능한 시각 모델 학습

## 개요

CLIP(Contrastive Language-Image Pre-training)은 OpenAI가 2021년 발표한 혁신적인 멀티모달 모델입니다. CLIP은 인터넷에서 수집한 4억 쌍의 이미지-텍스트 쌍을 활용하여, 이미지와 텍스트 사이의 의미적 관계를 대조 학습(contrastive learning) 방식으로 학습합니다. 이 접근 방식은 모델이 별도의 미세 조정 없이도 다양한 시각적 분류 태스크에서 뛰어난 제로샷(zero-shot) 성능을 발휘할 수 있게 해줍니다.

## 핵심 아이디어

CLIP의 주요 혁신점은 다음과 같습니다:

1. **자연어 지도 학습**: 특정 레이블 집합이 아닌, 자연어 설명을 통해 시각적 개념을 학습합니다. 이를 통해 모델은 훨씬 더 풍부하고 유연한 시각적 표현을 획득할 수 있습니다.

2. **대조 학습 접근 방식**: 이미지와 텍스트를 각각 인코딩한 후, 올바른 이미지-텍스트 쌍은 임베딩 공간에서 가깝게, 잘못된 쌍은 멀게 위치하도록 학습합니다.

3. **대규모 사전 학습**: 웹에서 수집한 4억 쌍의 이미지-텍스트 데이터로 훈련하여 다양한 시각적 개념을 포괄적으로 학습합니다.

4. **제로샷 전이**: 사전 학습된 CLIP은 새로운 시각적 개념을 텍스트 프롬프트로 설명하는 것만으로 인식할 수 있어, 별도의 미세 조정 없이 다양한 태스크에 적용 가능합니다.

## 모델 구조

CLIP은 두 개의 독립적인 인코더로 구성됩니다:

1. **이미지 인코더**: 시각적 표현을 학습하는 네트워크로, 논문에서는 두 가지 아키텍처를 실험했습니다:
   - ResNet-50, ResNet-101, ResNet-50x4, ResNet-50x16, ResNet-50x64
   - Vision Transformer (ViT-B/32, ViT-B/16, ViT-L/14)

2. **텍스트 인코더**: 텍스트 표현을 학습하는 네트워크로, 트랜스포머 기반 모델을 사용합니다.
   - 12층 또는 63M 파라미터의 트랜스포머

두 인코더는 이미지와 텍스트를 각각 동일한 차원의 임베딩 공간으로 매핑하며, 대조 학습을 통해 의미적으로 관련된 이미지-텍스트 쌍이 유사한 임베딩을 갖도록 학습됩니다.

## 학습 방식

CLIP의 학습 과정은 다음과 같습니다:

1. **배치 구성**: N개의 이미지-텍스트 쌍으로 이루어진 미니배치를 구성합니다.
2. **인코딩**: 이미지 인코더와 텍스트 인코더를 통해 각각의 이미지와 텍스트를 임베딩합니다.
3. **유사도 계산**: 모든 이미지-텍스트 쌍 사이의 코사인 유사도를 계산하여 NxN 유사도 행렬을 구성합니다.
4. **대조 손실 최적화**: 올바른 이미지-텍스트 쌍(대각선 요소)의 유사도는 최대화하고, 잘못된 쌍(비대각선 요소)의 유사도는 최소화하는 방향으로 모델을 최적화합니다.

## 주요 성과

CLIP은 다음과 같은 놀라운 성과를 보여주었습니다:

1. **강력한 제로샷 성능**: ImageNet에서 76.2%의 Top-1 정확도를 달성하며, 이는 많은 지도 학습 모델과 비교할 만한 수준입니다.

2. **일반화 능력**: CLIP은 ImageNet 분포 변화에 대한 강건성이 뛰어나며, 다양한 데이터셋에서 일관되게 좋은 성능을 보입니다.

3. **유연한 시각적 개념 학습**: 텍스트 프롬프트만으로 새로운 시각적 개념을 인식할 수 있어, 27개의 다양한 데이터셋에서 경쟁력 있는 성능을 보여줍니다.

4. **편향성 감소**: 일부 데이터셋에서 기존 지도 학습 모델보다 공정성 측면에서 개선된 성능을 보입니다.

## 영향 및 의의

CLIP의 등장은 컴퓨터 비전과 멀티모달 학습 분야에 큰 영향을 미쳤습니다:

1. **멀티모달 학습의 새 패러다임**: 자연어를 통한 시각적 개념 학습이라는 새로운 패러다임을 제시했습니다.

2. **제로샷 학습의 가능성 확장**: 사전 학습만으로 다양한 태스크에 적용 가능한 모델의 가능성을 보여주었습니다.

3. **생성 모델과의 시너지**: CLIP은 DALL-E, Stable Diffusion 등의 텍스트-이미지 생성 모델의 핵심 컴포넌트로 활용되었습니다.

4. **실용적 응용**: 이미지 검색, 시각적 질의응답, 이미지 캡셔닝 등 다양한 응용 분야에서 활용되고 있습니다.

## 한계점

CLIP의 주요 한계점으로는 다음과 같은 것들이 있습니다:

1. **복잡한 추론 능력 부족**: 단순한 시각적 개념 인식은 잘하지만, 복잡한 추론이나 세부적인 이해에는 제한이 있습니다.

2. **언어 기반 편향**: 훈련 데이터에 존재하는 언어적, 문화적 편향이 모델에 반영될 수 있습니다.

3. **고해상도 이미지 처리의 어려움**: 제한된 패치 크기로 인해 고해상도 이미지의 세부 정보 처리에 한계가 있습니다.

4. **연산 비용**: 대규모 모델의 경우 상당한 연산 리소스가 필요합니다.

## 결론

CLIP은 자연어 지도를 통한 시각적 표현 학습의 강력함을 보여주며, 대규모 웹 데이터를 활용한 사전 학습의 가능성을 확장했습니다. 제로샷 능력과 일반화 성능은 특히 인상적이며, 이는 향후 멀티모달 학습과 AI 시스템 개발에 중요한 이정표가 되었습니다.

CLIP이 제시한 접근 방식은 컴퓨터 비전의 패러다임을 '폐쇄형 세트 인식'에서 '열린 세트 이해'로 전환하는 데 기여했으며, 인간의 시각적 학습 방식에 더 가까운 AI 시스템으로 나아가는 중요한 단계로 평가받고 있습니다.

## 내가 이해한 포인트

CLIP의 가장 혁신적인 점은 거대한 규모의 웹 데이터를 활용해 언어와 이미지 간의 연결을 학습했다는 것입니다. 이 접근 방식은 기존의 지도 학습 모델들이 가진 '고정된 레이블 집합'이라는 제약을 극복했습니다.

특히 흥미로운 것은 CLIP이 단순히 이미지 분류에 그치지 않고, 텍스트 프롬프트를 통해 모델의 동작을 유연하게 제어할 수 있다는 점입니다. 예를 들어, "a photo of a dog"와 같은 단순한 프롬프트부터 "a pencil sketch of a dog"와 같이 스타일을 지정하는 프롬프트까지 다양하게 활용할 수 있습니다.

이러한 특성은 CLIP이 단순한 인식 모델을 넘어, 언어로 안내되는 시각적 이해 시스템으로서의 가능성을 보여줍니다. 이는 DALL-E, Stable Diffusion과 같은 생성 모델의 발전에도 큰 영향을 미쳤으며, 멀티모달 AI의 새로운 시대를 열었다고 생각합니다. 